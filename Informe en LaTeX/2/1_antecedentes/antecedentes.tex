El artículo de \cite{Joo2020} titulado "Traffic Signal Control for Smart Cities Using Reinforcement Learning" propone un sistema de control de señales de tráfico basado en aprendizaje por refuerzo, específicamente utilizando el algoritmo Q-learning (QL), para mejorar la gestión del tráfico en intersecciones dentro del contexto de ciudades inteligentes.

El problema principal abordado es la incapacidad de los sistemas tradicionales de control de tráfico con tiempos fijos para adaptarse a entornos dinámicos y datos complejos generados por las ciudades inteligentes. Estas limitaciones resultan en congestión, mayores tiempos de espera y un desequilibrio en la distribución de señales de tráfico, lo que afecta la eficiencia y equidad del sistema.

La metodología incluye la formulación de un modelo que utiliza Q-learning con un enfoque en el rendimiento (throughput) y la desviación estándar de las longitudes de cola como parámetros principales. Se diseñó un entorno simulado con SUMO para representar una intersección de cuatro vías, donde se evaluó el sistema mediante métricas de longitud de cola, desviación estándar de las colas y tiempos de espera promedio. Los estados se definieron en función del flujo vehicular por dirección, mientras que las recompensas se calcularon combinando la desviación estándar de las colas y el rendimiento mediante una función adaptativa.

Los resultados experimentales muestran que el algoritmo propuesto supera a modelos previos de Q-learning en todas las métricas evaluadas. En comparación con un modelo basado en tiempos fijos mejorado (E-TS) y un modelo basado en clústeres (C-TS), el sistema propuesto redujo la longitud de cola promedio en un 25\% frente a E-TS y un 63\% frente a C-TS. Además, disminuyó la desviación estándar de las colas en un 50\% y 75\%, respectivamente. En términos de tiempo de espera promedio por vehículo, el sistema logró una reducción del 15\% frente a E-TS y del 40\% frente a C-TS.

Estos resultados confirman que el sistema propuesto no solo es efectivo en reducir los tiempos de espera y equilibrar las señales, sino que también es escalable a diferentes configuraciones de intersecciones, mostrando potencial para implementaciones prácticas en ciudades inteligentes.

\cite{Li2021} presenta el artículo "Network-wide Traffic Signal Control Optimization Using a Multi-agent Deep Reinforcement Learning", donde se explora la problemática de la congestión vehicular en redes complejas de intersecciones múltiples, analizando cómo los enfoques tradicionales de control de tráfico, como los sistemas de tiempos fijos o descentralizados, no logran adaptarse dinámicamente a las condiciones cambiantes del tráfico. Estos métodos resultan en mayores tiempos de espera, congestión severa y un desperdicio significativo de recursos energéticos. El trabajo destaca la necesidad de enfoques más inteligentes y colaborativos para optimizar el control de señales de tráfico en redes urbanas extensas.

La metodología se centra en el desarrollo de un sistema basado en aprendizaje por refuerzo profundo multi-agente (KS-DDPG), que combina un aprendizaje centralizado con una ejecución descentralizada. Este sistema permite a los agentes en cada intersección tomar decisiones dinámicas basadas en un protocolo de compartición de conocimiento entre agentes. Las simulaciones del sistema se llevaron a cabo utilizando el simulador de tráfico SUMO, además de datos reales extraídos de redes de tráfico en Montgomery County, Maryland. Los indicadores clave de rendimiento incluyeron la longitud promedio de colas y el tiempo promedio de retraso por vehículo.

El modelo KS-DDPG integra un mecanismo de comunicación que permite a los agentes compartir experiencias relevantes y aprender colectivamente. Este enfoque colaborativo mejora significativamente la capacidad de los agentes para tomar decisiones óptimas sobre la duración de las fases de los semáforos, logrando así una sincronización eficiente en condiciones de tráfico fluctuantes. La arquitectura del modelo garantiza que el sistema sea escalable y eficiente, adaptándose a redes de tráfico complejas.

Los resultados del estudio demostraron que el modelo KS-DDPG supera ampliamente a otros métodos, como los sistemas de aprendizaje por refuerzo tradicional (DQN y MADDPG), y a los métodos de control de tráfico convencionales. Redujo notablemente la longitud de las colas y los tiempos de retraso, especialmente en escenarios de alta demanda vehicular, destacándose como una solución robusta para optimizar el flujo de tráfico en entornos urbanos.

El artículo de \cite{Li2021a} titulado "A Deep Reinforcement Learning Approach for Traffic Signal Control Optimization" aborda el desafío de optimizar las señales de tráfico en redes urbanas complejas utilizando un enfoque avanzado de aprendizaje por refuerzo profundo (DRL). El problema principal identificado es la ineficiencia de los sistemas de semaforización tradicionales y adaptativos, los cuales a menudo generan largos tiempos de espera y mayor congestión debido a su incapacidad para adaptarse dinámicamente a las condiciones fluctuantes del tráfico en tiempo real. Este trabajo introduce el algoritmo MADDPG (Multi-Agent Deep Deterministic Policy Gradient) como solución a estas limitaciones, combinando aprendizaje centralizado y ejecución descentralizada para abordar los retos de escalabilidad y coordinación en redes con múltiples intersecciones.

La metodología empleada se centra en un marco de Aprendizaje por Refuerzo Multiagente, modelando cada semáforo como un agente independiente en un entorno cooperativo-competitivo. Los datos del tráfico, como la longitud de las colas y el tiempo de espera, se obtienen mediante detectores en bucle y simulaciones con el software SUMO. Los agentes ajustan dinámicamente las fases de los semáforos basándose en observaciones locales y recompensas diseñadas para minimizar las demoras y optimizar el flujo vehicular. Durante el entrenamiento, el modelo utiliza una combinación de redes neuronales profundas (actor y crítico) para aprender políticas óptimas de control.

Los resultados del experimento, evaluados en un entorno de simulación que replica la red de tráfico del condado de Montgomery, Maryland, destacan la eficacia del algoritmo MADDPG en comparación con otros métodos como DQN y DDPG. En métricas clave como la longitud promedio de colas y el tiempo de espera por vehículo, MADDPG mostró una reducción significativa frente a los controladores de tiempo fijo y otros enfoques basados en DRL. Además, el sistema logró una mayor estabilidad en sus políticas de control, superando los problemas de no estacionariedad y alta varianza que suelen afectar a otros métodos.

El estudio concluye que MADDPG es una herramienta robusta y escalable para el control adaptativo de señales de tráfico en redes complejas. Sin embargo, también señala limitaciones relacionadas con la representación de estados en espacios de alta dimensionalidad y la necesidad de mejorar la eficiencia del aprendizaje en escenarios de datos limitados. Estos resultados refuerzan el potencial del aprendizaje por refuerzo profundo como una solución viable para los sistemas inteligentes de transporte en entornos urbanos.

El artículo de \cite{Mo2022} titulado "CVLight: Decentralized learning for adaptive traffic signal control with connected vehicles" presenta una metodología para optimizar el control de señales de tráfico en múltiples intersecciones mediante un esquema descentralizado de aprendizaje por refuerzo (RL), denominado CVLight. El principal problema que aborda este estudio es la dificultad de aplicar métodos tradicionales de control de semáforos a redes de tráfico dinámicas, donde los vehículos conectados (CV) pueden proporcionar información detallada y en tiempo real sobre el flujo vehicular. Sin embargo, los sistemas de control clásicos no logran aprovechar estos datos de manera eficiente, especialmente cuando la penetración de vehículos conectados es baja, lo que puede generar una optimización subóptima del flujo vehicular.

La metodología empleada en el artículo es un enfoque de aprendizaje por refuerzo descentralizado, utilizando un algoritmo novedoso llamado Asymmetric Advantage Actor-Critic (Asym-A2C), que emplea información parcial de los vehículos conectados (CVs) para entrenar el "actor" y la información completa (de CVs y vehículos no conectados) para entrenar el "crítico". En este esquema, los agentes (semáforos) se entrenan para seleccionar fases de semáforo de forma autónoma basándose en las observaciones que hacen de los vehículos presentes en cada intersección. Este enfoque permite que el sistema se adapte a situaciones de tráfico variadas, optimizando la duración de las fases de semáforo y reduciendo los retrasos en las intersecciones, incluso en escenarios con baja penetración de vehículos conectados.

Los experimentos realizados en el artículo utilizan una red de intersecciones de tamaño 2x2, con simulaciones que reflejan condiciones de tráfico dinámicas y variaciones en la tasa de penetración de CVs. Los resultados muestran que CVLight supera a los métodos tradicionales de control de semáforos y otros enfoques de aprendizaje por refuerzo, mostrando mejoras significativas en el tiempo de espera promedio por vehículo y en la eficiencia del tráfico. Específicamente, el modelo mostró una reducción del 30\% en el tiempo de espera promedio por vehículo con una penetración de CVs del 20\%, comparado con los modelos tradicionales. También se evaluó la escalabilidad del sistema, demostrando que el enfoque de preentrenamiento del modelo permite aplicar CVLight a redes de tráfico más grandes, como un sistema 5x5, con costos computacionales reducidos.

Los resultados de estos experimentos resaltan que el sistema CVLight no solo es efectivo en escenarios de alta penetración de CVs, sino que también ofrece una solución robusta cuando la penetración de CVs es baja, lo que lo convierte en una opción viable para el futuro de los sistemas de gestión del tráfico inteligente. Además, se destaca que el uso de un algoritmo de aprendizaje por refuerzo descentralizado permite que cada intersección aprenda de manera independiente, promoviendo la cooperación entre las intersecciones sin la necesidad de un sistema de control centralizado.

El artículo de \cite{Damadam2022} titulado "An Intelligent IoT Based Traffic Light Management System: Deep Reinforcement Learning" presenta un enfoque para el control adaptativo de señales de tráfico utilizando aprendizaje por refuerzo profundo y tecnologías IoT, aplicado en seis intersecciones de la ciudad de Shiraz, Irán.

El principal problema que aborda el artículo es la ineficiencia de los sistemas de control de tráfico basados en tiempos fijos, que no pueden adaptarse a las condiciones dinámicas del tráfico, lo que genera congestión, mayor consumo de combustible y contaminación ambiental. Además, estos sistemas presentan costos elevados de implementación y mantenimiento, lo que limita su viabilidad en ciudades metropolitanas.

La metodología incluye la implementación de un algoritmo de Aprendizaje por Refuerzo Multi-Agente (MARL, por sus siglas en inglés) combinado con el algoritmo Advantage Actor-Critic (A2C). Este sistema descentralizado utiliza sensores IoT como cámaras de vigilancia para recopilar datos de tráfico en tiempo real, como las longitudes de cola y los tiempos de espera de los vehículos, que son procesados localmente y compartidos entre agentes para optimizar las señales de tráfico. Además, se incorpora el método de huellas dactilares para estabilizar el aprendizaje entre agentes.

En términos de evaluación, los experimentos fueron realizados en un simulador de tráfico urbano (SUMO) utilizando datos reales de tráfico proporcionados por el municipio de Shiraz. Los resultados numéricos muestran una mejora significativa en la gestión del tráfico en comparación con el sistema de tiempos fijos existente. En promedio, el sistema propuesto logró reducir las longitudes de cola en un 20-30\% y los tiempos de espera de los vehículos en un 25\% en las seis intersecciones estudiadas.

Estos resultados destacan el potencial del uso combinado de técnicas de aprendizaje por refuerzo profundo y tecnologías IoT para mejorar la eficiencia del tráfico urbano, promoviendo un sistema de control robusto y escalable.

El artículo de \cite{Cao2024} titulado "Optimization Control of Adaptive Traffic Signal with Deep Reinforcement Learning" propone una metodología para optimizar el control adaptativo de señales de tráfico utilizando un algoritmo mejorado basado en aprendizaje por refuerzo profundo denominado G-DQN, implementado y evaluado mediante simulaciones en el software SUMO.

El problema principal identificado es la ineficacia de los esquemas tradicionales de control de tráfico con tiempos fijos, que no pueden adaptarse a las condiciones de tráfico dinámico. Esto resulta en mayores tiempos de espera, congestión, y una gestión subóptima del tráfico en intersecciones complejas. Adicionalmente, las estrategias de aprendizaje por refuerzo existentes presentan limitaciones en la representación de estados y recompensas, así como en la convergencia eficiente de los modelos.

La metodología se basa en el desarrollo del algoritmo G-DQN, que introduce mejoras en la estructura de redes neuronales convolucionales (CNN) para procesar matrices duales de estado (posición y velocidad de vehículos), y redefine los estados y recompensas para reflejar con mayor precisión las condiciones de tráfico. También utiliza un enfoque dinámico para la selección de acciones con el método e-greedy y simula datos de tráfico usando la distribución Weibull para emular patrones de tráfico reales.

En los experimentos, se compararon los resultados del G-DQN con el algoritmo DQN original, el algoritmo A2C y un esquema de tiempo fijo. Los resultados numéricos muestran que el G-DQN redujo el tiempo acumulado en cola a 26,676 segundos, frente a 38,870 del DQN y 40,538 del esquema fijo, el promedio de longitud de cola fue de 4.9 vehículos para el G-DQN, comparado con 7.2 del DQN y 8.0 del esquema fijo, y el G-DQN logró la convergencia en 3200 pasos, más rápido que el DQN (4100 pasos) y comparable al A2C (3600 pasos).

Estos resultados demuestran que el G-DQN mejora significativamente la eficiencia del tráfico en intersecciones simuladas, optimizando tanto el tiempo de espera como la longitud de las colas en comparación con los métodos tradicionales y otros enfoques de aprendizaje por refuerzo.

El artículo de \cite{Gu2021} titulado "Traffic Signal Optimization for Multiple Intersections Based on Reinforcement Learning" propone un modelo de optimización de señales de tráfico basado en aprendizaje por refuerzo profundo, diseñado para operar bajo restricciones reales de intersecciones múltiples. El modelo mantiene la secuencia típica de fases de semáforo y considera un tiempo mínimo de luz verde, aspectos clave para su aplicabilidad en entornos reales.

El problema principal abordado en el estudio es la limitación de los modelos tradicionales de control de señales fijas y de algunos modelos de aprendizaje por refuerzo existentes, que a menudo ignoran restricciones operativas como la secuencia fija de fases y el tiempo mínimo de luz verde. Estos enfoques pueden resultar en configuraciones no viables, causando confusión entre los conductores y vehículos con tiempos de espera excesivos.

La metodología se centra en un modelo basado en Deep Q-Network (DQN), entrenado en un entorno simulado con SUMO (Simulation of Urban MObility). Los estados del modelo incluyen la cola de vehículos por carril, la fase activa del semáforo y el tiempo transcurrido de la fase activa. Las acciones posibles son mantener la fase actual o cambiar a la siguiente, siempre respetando las restricciones de tiempo mínimo verde y secuencia de fases. La recompensa se define como el cociente entre los vehículos que pasan y los detenidos durante un intervalo de tiempo.

En los experimentos realizados, se evaluaron dos escenarios. En el primero, con dos intersecciones conectadas, el modelo propuesto redujo el retraso promedio por vehículo de 40 segundos a 30 segundos y el número promedio de paradas de 2.5 a 2, comparado con un modelo fijo. En el segundo escenario, que incluye seis intersecciones en una red realista, se obtuvo una reducción del 88\% al 31\% en el retraso acumulado y del 95\% al 46\% en el número de paradas acumuladas frente al modelo fijo. Durante las horas pico, el retraso promedio se redujo de 3 minutos y 15 segundos a 2 minutos y 15 segundos, mientras que el número de paradas promedio por vehículo disminuyó de 11 a 4.7.

Estos resultados destacan que, aunque el modelo propuesto no supera en todas las métricas a un modelo de comparación sin restricciones, ofrece una solución más realista y aplicable para su implementación en el mundo real, asegurando un equilibrio entre eficiencia y viabilidad práctica.

El artículo de \cite{Rasheed2020} titulado "Deep Reinforcement Learning for Traffic Signal Control: A Review" presenta una revisión exhaustiva sobre el uso del aprendizaje por refuerzo profundo (DRL) para el control de señales de tráfico, abarcando arquitecturas, plataformas de simulación, análisis de complejidad, y métricas de rendimiento.

El principal problema que aborda es la complejidad creciente en la gestión del tráfico urbano debido al aumento de la congestión vehicular, causada por el crecimiento poblacional y la urbanización. Los métodos tradicionales de control de tráfico enfrentan limitaciones como el costo computacional elevado y la incapacidad para adaptarse a entornos dinámicos y no estructurados, especialmente en redes de tráfico densas.

La metodología revisa las arquitecturas más comunes de DRL aplicadas al control de tráfico, incluyendo redes neuronales convolucionales (CNN), redes neuronales totalmente conectadas (FCLN), y redes LSTM, explorando sus capacidades para representar espacios de estado complejos y optimizar señales de tráfico. Además, se analizan las plataformas de simulación, como SUMO y Aimsun, que permiten modelar redes de tráfico realistas y evaluar los algoritmos propuestos.

Los resultados muestran que las soluciones basadas en DRL superan significativamente a los enfoques tradicionales en métricas clave como tiempo promedio de espera, longitud de cola promedio y rendimiento (throughput). Por ejemplo, se reporta que algunas implementaciones logran reducir el tiempo de espera promedio en un 15-30\% en comparación con sistemas de tiempo fijo, dependiendo del escenario de simulación y la arquitectura de DRL utilizada.

El artículo también identifica desafíos abiertos, como la escalabilidad de los modelos para redes de múltiples intersecciones y la integración de datos de tráfico en tiempo real, planteando direcciones futuras de investigación.

El artículo de \cite{Zhu2022} titulado "Traffic Sign Recognition Based on Deep Learning" presenta un análisis comparativo entre los modelos YOLOv5 y SSD para la detección y reconocimiento de señales de tráfico, evaluando su rendimiento en términos de precisión, velocidad y adaptabilidad en un conjunto de datos creado específicamente para este estudio.

El problema principal identificado es la incapacidad de los métodos tradicionales de reconocimiento de señales de tráfico para manejar escenarios complejos en tiempo real, como cambios en la iluminación, ángulos de cámara y oclusiones. Esto limita su aplicabilidad en sistemas de transporte inteligente y vehículos autónomos.

La metodología empleada incluye la recopilación de un conjunto de datos personalizado con 2,182 imágenes de señales de tráfico organizadas en ocho clases. Se implementaron experimentos utilizando YOLOv5 y SSD en la plataforma Google Colab con GPU de alto rendimiento, evaluando los modelos mediante métricas como precisión promedio (mAP) y velocidad de reconocimiento (FPS).

En los resultados, YOLOv5 superó significativamente a SSD en precisión y velocidad. YOLOv5 alcanzó un mAP promedio de 97.70\% para todas las clases, mientras que SSD logró un 90.14\%. En términos de velocidad, YOLOv5 procesó imágenes a 30 FPS, mientras que SSD lo hizo a 3.49 FPS, lo que indica que YOLOv5 es aproximadamente 10 veces más rápido.

El estudio concluye que YOLOv5 es más adecuado para entornos de tráfico en tiempo real debido a su alta precisión y velocidad, destacando su potencial para aplicaciones en sistemas de transporte inteligente.

